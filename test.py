# --- 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import torch
import os

# --- 1. ë°ì´í„° ì¤€ë¹„ ---
print("--- 1. ë°ì´í„° ì¤€ë¹„ ì‹œì‘ ---")

# ê° íŒŒì¼ ê²½ë¡œë¥¼ ì§ì ‘ í•œ ì¤„ë¡œ ì§€ì •
HUMAN_FILE = 'human_datasets/processed_human.json'
AI_FILE = 'ai_datasets/processed_gpt.json'

# ê° JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    with open(HUMAN_FILE, 'r', encoding='utf-8') as f:
        human_texts = json.load(f)
    with open(AI_FILE, 'r', encoding='utf-8') as f:
        ai_texts = json.load(f)
except FileNotFoundError as e:
    print(f"ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼({e.filename})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— human_datasetsì™€ ai_datasets í´ë” ë° íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„± (í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¬¶ì–´ì¤Œ)
data = []
for text in human_texts:
    data.append({'text': text, 'label': 0}) # ì‚¬ëŒì€ ë¼ë²¨ 0
for text in ai_texts:
    data.append({'text': text, 'label': 1}) # AIëŠ” ë¼ë²¨ 1

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ í›„ ë¬´ì‘ìœ„ë¡œ ì„ê¸°
df = pd.DataFrame(data)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

print("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
print(f"ì´ ë°ì´í„° ê°œìˆ˜: {len(df)}")
print(f"ì‚¬ëŒ ë°ì´í„° ê°œìˆ˜: {len(human_texts)}")
print(f"AI ë°ì´í„° ê°œìˆ˜: {len(ai_texts)}")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬
train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)

# --- 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ---
print("\n--- 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘ ---")
MODEL_NAME = "skt/kobert-base-v1"
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print("ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

# --- 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ---
class KoBERTDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.text = dataframe.text.values
        self.labels = dataframe.label.values
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        label = self.labels[index]
        encoding = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.max_len,
            return_token_type_ids=False, padding='max_length',
            truncation=True, return_attention_mask=True, return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# --- 4. ì„¤ì •ê°’ ì •ì˜ ---
MAX_LEN = 256
BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 2e-5
SAVE_PATH = "./kobert_ai_human_classifier.pth"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# --- 5. ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ë¶ˆëŸ¬ì˜¤ê¸° ---
if os.path.exists(SAVE_PATH):
    print(f"\n--- ì €ì¥ëœ ëª¨ë¸({SAVE_PATH})ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ ---")
    model.load_state_dict(torch.load(SAVE_PATH, map_location=device))
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
else:
    print("\n--- ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    train_dataset = KoBERTDataset(train_df, tokenizer, MAX_LEN)
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    # 6. ëª¨ë¸ í•™ìŠµ
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for batch in tqdm(train_dataloader, desc=f"Epoch {epoch + 1}/{EPOCHS}"):
            optimizer.zero_grad()
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()
            optimizer.step()
        avg_train_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch + 1} | Training Loss: {avg_train_loss:.4f}")

    # 7. ëª¨ë¸ í‰ê°€
    test_dataset = KoBERTDataset(test_df, tokenizer, MAX_LEN)
    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
    model.eval()
    total_eval_accuracy = 0
    for batch in tqdm(test_dataloader, desc="Evaluating"):
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            total_eval_accuracy += (predictions == labels).sum().item()
    accuracy = total_eval_accuracy / len(test_df)
    print(f"Accuracy: {accuracy:.4f}")

    # 8. í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), SAVE_PATH)
    print(f"ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ. ëª¨ë¸ì´ {SAVE_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 9. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ---
def predict(text):
    model.eval()
    encoding = tokenizer.encode_plus(
        text, add_special_tokens=True, max_length=MAX_LEN, return_token_type_ids=False,
        padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'
    )
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=-1)
    human_prob = probs[0][0].item()
    ai_prob = probs[0][1].item()
    prediction = "ì‚¬ëŒì´ ì‘ì„±í•œ ê¸€" if human_prob > ai_prob else "AIê°€ ì‘ì„±í•œ ê¸€"
    
    print(f"\nì…ë ¥ í…ìŠ¤íŠ¸: \"{text}\"")
    print("-" * 50)
    print(f"ğŸ¤– AIì¼ í™•ë¥ : {ai_prob*100:.2f}%")
    print(f"ğŸ§‘â€ğŸ’» ì‚¬ëŒì¼ í™•ë¥ : {human_prob*100:.2f}%")
    print(f"==> ìµœì¢… íŒë³„: {prediction}")
    print("-" * 50)

# --- 10. ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ---
print("\n--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
while True:
    user_input = input("\níŒë³„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'ì¢…ë£Œ' ë˜ëŠ” 'exit' ì…ë ¥): ")
    if user_input.lower() in ['ì¢…ë£Œ', 'exit']:
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    predict(user_input)